# File Compression

- 大一高程大作业：文件压缩
- Programing assignment file compression in 2019 Fall

## 设计思路⚡

### 数据压缩原理

数据压缩（Data Compress）的原理是按照一定的算法对数据进行重新组织，减少数据的冗余和存储的空间。

常见的无损数据压缩算法有LZW压缩，LZ77压缩，Huffman编码压缩等等。

### 选择压缩算法

当对算法的选择无从下手时，我选择了先观察大作业给出的目标日志文件（ser.log），通过文件的特点选择相应的压缩算法。毕竟有一些算法还是比较难实现的，就算实现了，要真正理解起来还是有一定困难的。

由此可以看出此文件包含着大量重复的冗余的字符串信息。那么参考之前提到的数据压缩的方法，最好的选择便是基于字典的压缩算法，而不选择Huffman编码算法，因为Huffman编码与基于字典的算法在维度上就有不同，前者侧重的是全局的字符层面的冗余，而后者侧重的是局部语句层面的冗余。当然在能力允许的范围内最好是结合这两种算法（比如高效的压缩方法GZIP），但是出于实现的难度，我最终选择了**GZIP中的LZ77算法作为主要压缩方法**。

### LZ77算法具体流程

> 了解哈希表：
> 哈希表是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。在这里，哈希表的key值就是三字符的字符串，而得到的位置就是该字符串在当前窗口中的相对位置。
> 本算法中的哈希表head数组的大小为32K，为了解决哈希冲突的问题（即当新出现的字符串的哈希地址与之前的字符串相同而导致值被覆盖，从而影响匹配的效率），引进了一个prev数组（32K），用此数组与head数组形成一种链式结构的匹配链，以此来提高查找匹配效率

#### 压缩过程

- 打开待压缩文件（二进制打开）
- 读取一个窗口的数据（64K）
- 通过开始两个字符初始化哈希地址，之后每读一个字符就可以将此字符和其前面两个字符组成的字符串的哈希地址计算出来
- 循环开始压缩
  *1 计算哈希地址，将该字符串首字符在窗口中的位置插入到哈希表中，并返回该表的状态matchHead
  *2 根据matchHead检测是否找到匹配
    +1 matchHead等于0，表示未找到匹配，该三个字符在前文中没有出现过，将该当前字符作为源字符写到压缩文件
    +2 matchHead不等于0，表示找到匹配，matchHead代表匹配链的首地址，从哈希桶matchHead位置开始找最长匹配，找到后用该<长度,距离>对替换该字符串写到压缩文件中，然后将该替换串三个字符一组添加到哈希表中
- 如果窗口中的数据小于MIN_LOOKAHEAD时，将右窗口中数据搬移到左窗口，从文件中新读取一个窗口的数据放置到右窗，更新哈希表，继续压缩，直到压缩结束

#### 压缩数据保存

压缩数据分为两个文件保存
+1 压缩数据，用以保存压缩后的数据或者<长度,距离>数据
+2 标记信息，用以保存标记当前字节是源字符还是<长度,距离>对

#### 解压缩过程

- 从标记文件中读取标记
- 如果当前标记是0，表示源字符，从压缩数据文件中读取一个字节，直接写到解压缩之后的文件中
- 如果当前标记是1，表示遇到<长度,距离>，从压缩数据文件中先读取一个字节作为压缩的长度，再继续读取两个字节作为距离，然后从解压缩过的结果中找出匹配长度
- 获取下一个标记，直到所有的标记读取完

#### 图解

每次读取一个字符，通过 该字符 和 哈希函数 计算出以该字符结尾的三字符字符串的哈希地址s，若head[s]为空，则代表前面没有该字符串的匹配串，若head[s]不为空，则说明之前至少有一个匹配串，于是就要查找最长匹配

用以上的例子说明，通过计算"abb"的哈希地址，发现之前有匹配串，并且得到了一个匹配链，再通过该匹配链找出最长匹配串，如图，最长的匹配链应该为”abbacd“，于是就将当前字符串”abbacd“替换为<长度，距离>=<6,9>写入到压缩文件中，并且向标记文件中添加标记位’1‘，再将”abbacd“三个字符为一组插入到哈希表中。当前读取位置移动到'f'处，当LOOK_AHEAD的大小小于MIN_LOOKAHEAD时，将窗口滑动32K

解压缩过程就是通过标志位来确定是否要读取为<长度，距离>对，如图，当标志位为’1‘时，读取<6,9>,将距离 ’e‘ 9个字符处开始，长度为6的字符串”abbacd“写入解压后文件

### LZ77算法实现

#### 哈希表类的实现

**哈希表**是GZIP中LZ77算法提高效率的关键，合理地选择哈希表的大小和匹配最大匹配次数对于匹配效率有很大的影响。
哈希表的主要功能就是保存之前出现过的字符串（长度为3）在窗口中的相对位置以及将同样的字符串（长度为3）形成匹配链以确保找到最长匹配长度的串。其中最主要的函数为哈希函数与插入函数：

> HUSHMASK详解:
> 查找缓冲区最多有32K，数组Prev[]共有32K个元素，而该数组索引的实际意义又是字符串的位置，所以我们能将该数组与查找缓冲区联系起来。我们知道，原始数据是放到Window中的，Window大小为64K；先行缓冲区的第一个字节strStart随着压缩的逐字节进行而在Window中不断推进，势必要大于32K（strStart的范围是闭区间[0, 64*1024- MIN_LOOKAHEAD]，前面提到过一点），而strStart的位置其实就是当前字符串的位置。prev[]只有32K个元素，其索引就是字符串的位置，而strStart如果大于32K，那还怎么把字符串插到字典中？prev[strStart]就数组越界了,为了解决这个问题，压缩算法内部对prev[]的索引使用了一个非常巧妙而又简单易懂的处理方法：让strStart与HUSHMASK按位与运算的结果作prev[]的索引，而不是直接用strStart作prev[]的索引，其中，HUSHMASK的值是32768，即prev[strStart & HUSHMASK]。这样就保证了在strStart的取值范围内，prev[]的索引与strStart都是完全对应的。例如，strStart = 300，那么strStart & HUSHMASK = 300；strStart = 32968 = 32K + 200，那么strStart &HUSHMASK= 200。这么看来，prev[]数组可以做到在strStart超过32K的时候，始终保证匹配链的长度在32K的范围内，用新的字符串位置值替换掉同一索引下旧的字符串位置值（旧的位置与当前字符串的位置之间的距离有可能已经超过32K，就算不超过也是所有匹配串中最远的）

此外还有需要实现的函数有”更新哈希表“（在每一次窗口滑动之后都要更新哈希表），”获得下一个匹配串位置“（查找匹配过程中，通过Prev数组的匹配链不断获取匹配串位置）

## 功能描述💬

### 对不同文件的压缩

| 文件类型    | 文件大小 / 重复度 | 压缩时间 | 压缩比(压缩文件大小/原文件大小) | 解压时间（待优化） |
| :---------- | ----------------- | -------- | ------------------------------- | ------------------ |
| 文本文件    | 10073KB / 较高    | 1.625 s  | 4.7%                            | 27.234 s           |
| 文本文件    | 4925KB / 低       | 2.078 s  | 97%                             | 12.187 s           |
| png图片文件 | 650KB / ~         | 0.25 s   | 84%                             | 1.109 s            |

### 压缩效率分析

对于一般的文本文件（文本重复度适中），可以达到50%的压缩比，但是当文本文件的重复度极低时，压缩效率就很不乐观，有时甚至出现压缩文件大于原文件的情况，而对于图片文件，压缩效率普遍不怎么理想。由此分析**GZIP中的LZ77算法对文件的重复程度依赖性很高，如果再结合Huffman算法压缩**，其压缩就会更加高效。

## 路遇问题与解决办法🚫

### 查找匹配串进入死循环✅

> 查找最长匹配的过程原理：
> 当获得的Head[hashAdress]不为空时(即不为0)时，我们需要返回这个链式结构的头（匹配头），然后再沿着这个链式结构寻找最长的匹配链，直到该链某节点的数值为空（即为0）时，停止匹配，获得最长匹配链对于的<长度，距离>对。

+1 问题：之前我们介绍HUSHMASK的时候说到过，当使用新的位置覆盖prev[strStart&HUSHMASK]的时候，这个被覆盖的值有可能是0，即“再往前没有匹配串”。如果没被覆盖，搜索匹配链时如果到了这个节点，就不会继续再搜了；但是现在被覆盖了，也就是说表示“再往前没有匹配串”这个语义的节点不存在了，那查找就会陷入死循环。
+2 解决办法：通过设置一个最大匹配次数，记为maxMatchTimes ，每次沿着匹配链查找的时候，最多找maxMatchTimes 次。这样不仅能够解决这种潜在的死循环问题，还可以通过设置maxMatchTimes 为不同的值来掌控压缩的效率。

### 解压缩效率不高❌

+1 解压过程很好理解，从标记文件中读取标记，为0则直接原字符输入，为1则读取一个<长度l，距离d>对，**把距离当前位置之前d处的长度为l的字符串写入文件中**。由于大作业要求使用fstream处理文件，所以我先使用了fstream对同一个文件进行读写，但是效果并不乐观（特别是当标记为1时，获取<长度,距离>对的过程是正确的，但是写入文件一直出问题），大多情况下都输出乱码或者二进制文件（如下图）：

+1 不知道是文件流指针的原因还是某某原因，让我一直很困扰，debug了很久🙃
+2 最后勉强使用了c语言方式的FILE*的方式，每次读取一个字符，然后**清空缓冲区**再写入（如前文UnCompressFile函数）
  解压缩的时间的确比较长，所以还需要优化，之前有许多思路，但是都以乱码告终，可能是自己脑子瓦特了😫

## 心得体会🖤

### 关于算法

+1 通过这次大作业，让我对算法这东西有了一点初步的了解，因为现在还没有上过数据结构有关的课，没有深入接触过算法，所以这次作业对我来说还是有很大的挑战性的👍。
+2 有一些算法是真的需要很长的时间（对我来说）才可以理解透的，理解了之后还要处理细节部分，最终还要达到可以运用在实际程序中的水平😥。
+3 算法学习真的是核心，语言学习是基础，”语言是不同的招式，而算法是内功“，有一说一，就算把c++撸完也不可能徒手把这个大作业写出来，一定要借助参考文献和相关算法流程📑.

### 关于程序

+1 我们日常使用的压缩程序（7-zip，zip，WinRAR...）实际上运用了如此高阶的算法，而且大多都是许多算法结合，优势互补，达到最大优化🏆。
+2 肝一个程序的过程是及其艰难的，有时可能就是因为一点小bug过不去降低了整个程序的可用性，我什么时候才可以成为一个人肝一个程序的大佬啊！当然，团队还是最重要的，多一个人，多一份智慧，多一份力量（但大作业要独立完成）💪！
